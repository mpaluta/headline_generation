{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mpaluta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mpaluta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from statistics import mean \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1988/08/16/0171246.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.py        A class for parsing the raw XML files\n",
    "#      |___ utilities.py         Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ __init__.py          Required file for t2t\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpaluta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n",
      "/home/mpaluta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/home/mpaluta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=100,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster helper functions for sentiment analysis\n",
    "\n",
    "sentiment_dict = {} # stores tokens with their sentiment score for quick lookup\n",
    "\n",
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment2(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment2(words):\n",
    "    log_odds = [] # holds log odds\n",
    "    for word in words: # if we've seen this word before, look up the score in dictionary rather than model\n",
    "        if word in sentiment_dict:\n",
    "            log_odds.append(sentiment_dict[word])\n",
    "        else: # if we haven't seen word before, score it with model and add to dictionary for next time\n",
    "            score = vecs_to_sentiment2(embeddings.loc[[word]].dropna())[0]\n",
    "            sentiment_dict[word] = score\n",
    "            log_odds.append(score)\n",
    "    return log_odds\n",
    "\n",
    "def text_to_sentiment2(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment2(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return mean(sentiments)\n",
    "\n",
    "def score_article2(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment2(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10000 after 227.39887690544128 seconds.\n",
      "Processing file 20000 after 380.47605538368225 seconds.\n",
      "Processing file 30000 after 509.00288128852844 seconds.\n",
      "Processing file 40000 after 636.9675922393799 seconds.\n",
      "Processing file 50000 after 754.4853117465973 seconds.\n",
      "Processing file 60000 after 870.5374150276184 seconds.\n",
      "Processing file 70000 after 992.1892170906067 seconds.\n",
      "Processing file 80000 after 1105.5715804100037 seconds.\n",
      "Processing file 90000 after 1226.2673852443695 seconds.\n",
      "Processing file 100000 after 1344.4723536968231 seconds.\n",
      "Processing file 110000 after 1453.0063469409943 seconds.\n",
      "Processing file 120000 after 1575.176667690277 seconds.\n",
      "Processing file 130000 after 1688.8246533870697 seconds.\n",
      "Processing file 140000 after 1795.362735748291 seconds.\n",
      "Processing file 150000 after 1902.7133774757385 seconds.\n",
      "Processing file 160000 after 2004.6912467479706 seconds.\n",
      "Processing file 170000 after 2121.3537130355835 seconds.\n",
      "Processing file 180000 after 2230.6022460460663 seconds.\n",
      "Processing file 190000 after 2335.4182641506195 seconds.\n",
      "Processing file 200000 after 2453.5682396888733 seconds.\n",
      "Processing file 210000 after 2556.912448644638 seconds.\n",
      "Processing file 220000 after 2663.2845125198364 seconds.\n",
      "Processing file 230000 after 2775.1183536052704 seconds.\n",
      "Processing file 240000 after 2877.5347006320953 seconds.\n",
      "Processing file 250000 after 2983.987530231476 seconds.\n",
      "Processing file 260000 after 3085.7283816337585 seconds.\n",
      "Processing file 270000 after 3189.358300447464 seconds.\n",
      "Processing file 280000 after 3301.0679156780243 seconds.\n",
      "Processing file 290000 after 3400.257693529129 seconds.\n",
      "Processing file 300000 after 3518.1075234413147 seconds.\n",
      "Processing file 310000 after 3618.152582883835 seconds.\n",
      "Processing file 320000 after 3720.4399206638336 seconds.\n",
      "Processing file 330000 after 3837.8114969730377 seconds.\n",
      "Processing file 340000 after 3947.4759480953217 seconds.\n",
      "Processing file 350000 after 4055.8216807842255 seconds.\n",
      "Processing file 360000 after 4154.126286268234 seconds.\n",
      "Processing file 370000 after 4253.752846956253 seconds.\n",
      "Processing file 380000 after 4360.4955859184265 seconds.\n",
      "Processing file 390000 after 4446.145461559296 seconds.\n",
      "Processing file 400000 after 4543.920250654221 seconds.\n",
      "Processing file 410000 after 4637.851725101471 seconds.\n",
      "Processing file 420000 after 4732.255117416382 seconds.\n",
      "Processing file 430000 after 4821.2179663181305 seconds.\n",
      "Processing file 440000 after 4910.444593667984 seconds.\n",
      "Processing file 450000 after 4999.621219873428 seconds.\n",
      "Processing file 460000 after 5086.522988319397 seconds.\n",
      "Processing file 470000 after 5181.161972522736 seconds.\n",
      "Processing file 480000 after 5273.801683187485 seconds.\n",
      "Processing file 490000 after 5369.680063009262 seconds.\n",
      "Processing file 500000 after 5463.987534999847 seconds.\n",
      "Processing file 510000 after 5559.507716417313 seconds.\n",
      "Processing file 520000 after 5653.664684295654 seconds.\n",
      "Processing file 530000 after 5741.8758289813995 seconds.\n",
      "Processing file 540000 after 5831.574619054794 seconds.\n",
      "Processing file 550000 after 5923.155802488327 seconds.\n",
      "Processing file 560000 after 6016.242149353027 seconds.\n",
      "Processing file 570000 after 6108.579118728638 seconds.\n",
      "Processing file 580000 after 6201.75629901886 seconds.\n",
      "Processing file 590000 after 6292.707636356354 seconds.\n",
      "Processing file 600000 after 6386.6588587760925 seconds.\n",
      "Processing file 610000 after 6476.946414709091 seconds.\n",
      "Processing file 620000 after 6571.002614736557 seconds.\n",
      "Processing file 630000 after 6663.6724944114685 seconds.\n",
      "Processing file 640000 after 6760.548567533493 seconds.\n",
      "Processing file 650000 after 6855.842997789383 seconds.\n",
      "Processing file 660000 after 6950.366146802902 seconds.\n",
      "Processing file 670000 after 7044.716697216034 seconds.\n",
      "Processing file 680000 after 7136.758740663528 seconds.\n",
      "Processing file 690000 after 7224.813081502914 seconds.\n",
      "Processing file 700000 after 7317.297978878021 seconds.\n",
      "Processing file 710000 after 7410.3712866306305 seconds.\n",
      "Processing file 720000 after 7502.000865221024 seconds.\n",
      "Processing file 730000 after 7596.213660955429 seconds.\n",
      "Processing file 740000 after 7686.118591547012 seconds.\n",
      "Processing file 750000 after 7776.204295396805 seconds.\n",
      "Processing file 760000 after 7862.053026914597 seconds.\n",
      "Processing file 770000 after 7947.94523024559 seconds.\n",
      "Processing file 780000 after 8033.380851268768 seconds.\n",
      "Processing file 790000 after 8122.367354393005 seconds.\n",
      "Processing file 800000 after 8208.773873090744 seconds.\n",
      "Processing file 810000 after 8293.333663225174 seconds.\n",
      "Processing file 820000 after 8377.507570266724 seconds.\n",
      "Processing file 830000 after 8465.047830581665 seconds.\n",
      "Processing file 840000 after 8553.884860754013 seconds.\n",
      "Processing file 850000 after 8641.493500232697 seconds.\n",
      "Processing file 860000 after 8725.957063674927 seconds.\n",
      "Processing file 870000 after 8812.321689605713 seconds.\n",
      "Processing file 880000 after 8911.709482431412 seconds.\n",
      "Processing file 890000 after 9013.77376484871 seconds.\n",
      "Processing file 900000 after 9109.213678836823 seconds.\n",
      "Processing file 910000 after 9190.859862565994 seconds.\n",
      "Processing file 920000 after 9271.920538425446 seconds.\n",
      "Processing file 930000 after 9351.763845920563 seconds.\n",
      "Processing file 940000 after 9434.010796546936 seconds.\n",
      "Processing file 950000 after 9516.406420469284 seconds.\n",
      "Processing file 960000 after 9595.660653352737 seconds.\n",
      "Processing file 970000 after 9679.964512825012 seconds.\n",
      "Processing file 980000 after 9763.374060153961 seconds.\n",
      "Processing file 990000 after 9843.941251516342 seconds.\n",
      "Processing file 1000000 after 9927.711411714554 seconds.\n",
      "Processing file 1010000 after 10010.183501005173 seconds.\n",
      "Processing file 1020000 after 10092.319759607315 seconds.\n",
      "Processing file 1030000 after 10176.394103050232 seconds.\n",
      "Processing file 1040000 after 10258.182426214218 seconds.\n",
      "Processing file 1050000 after 10342.305479049683 seconds.\n",
      "Processing file 1060000 after 10426.023347377777 seconds.\n",
      "Processing file 1070000 after 10508.470140218735 seconds.\n",
      "Processing file 1080000 after 10587.923907279968 seconds.\n",
      "Processing file 1090000 after 10668.350056171417 seconds.\n",
      "Processing file 1100000 after 10749.75277709961 seconds.\n",
      "Processing file 1110000 after 10835.875843048096 seconds.\n",
      "Processing file 1120000 after 10912.823616743088 seconds.\n",
      "Processing file 1130000 after 10992.337374448776 seconds.\n",
      "Processing file 1140000 after 11073.582974910736 seconds.\n",
      "Processing file 1150000 after 11156.887428045273 seconds.\n",
      "Processing file 1160000 after 11239.596583604813 seconds.\n",
      "Processing file 1170000 after 11322.066526651382 seconds.\n",
      "Processing file 1180000 after 11405.257844209671 seconds.\n",
      "Processing file 1190000 after 11503.886291980743 seconds.\n",
      "Processing file 1200000 after 11602.465003967285 seconds.\n",
      "Processing file 1210000 after 11701.547166347504 seconds.\n",
      "Processing file 1220000 after 11798.634886264801 seconds.\n",
      "Processing file 1230000 after 11893.041691303253 seconds.\n",
      "Processing file 1240000 after 11994.799726963043 seconds.\n",
      "Processing file 1250000 after 12097.942452430725 seconds.\n",
      "Processing file 1260000 after 12197.52063536644 seconds.\n",
      "Processing file 1270000 after 12295.467037677765 seconds.\n",
      "Processing file 1280000 after 12395.674699544907 seconds.\n",
      "Processing file 1290000 after 12493.896533250809 seconds.\n",
      "Processing file 1300000 after 12590.103939056396 seconds.\n",
      "Processing file 1310000 after 12688.57666349411 seconds.\n",
      "Processing file 1320000 after 12788.354401826859 seconds.\n",
      "Processing file 1330000 after 12888.819647073746 seconds.\n",
      "Processing file 1340000 after 12990.275651454926 seconds.\n",
      "Processing file 1350000 after 13090.960619449615 seconds.\n",
      "Processing file 1360000 after 13190.197250127792 seconds.\n",
      "Processing file 1370000 after 13288.585556268692 seconds.\n",
      "Processing file 1380000 after 13387.332028388977 seconds.\n",
      "Processing file 1390000 after 13487.694156646729 seconds.\n",
      "Processing file 1400000 after 13590.676194429398 seconds.\n",
      "Processing file 1410000 after 13693.05419754982 seconds.\n",
      "Processing file 1420000 after 13794.480812549591 seconds.\n",
      "Processing file 1430000 after 13896.925398111343 seconds.\n",
      "Processing file 1440000 after 14002.97982597351 seconds.\n",
      "Processing file 1450000 after 14107.122004032135 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1460000 after 14208.912722349167 seconds.\n",
      "Processing file 1470000 after 14311.707476854324 seconds.\n",
      "Processing file 1480000 after 14419.654128551483 seconds.\n",
      "Processing file 1490000 after 14523.6739320755 seconds.\n",
      "Processing file 1500000 after 14627.506765842438 seconds.\n",
      "Processing file 1510000 after 14728.519894361496 seconds.\n",
      "Processing file 1520000 after 14831.380182027817 seconds.\n",
      "Processing file 1530000 after 14933.77419424057 seconds.\n",
      "Processing file 1540000 after 15035.65598320961 seconds.\n",
      "Processing file 1550000 after 15135.216666221619 seconds.\n",
      "Processing file 1560000 after 15236.4183011055 seconds.\n",
      "Processing file 1570000 after 15339.125453710556 seconds.\n",
      "Processing file 1580000 after 15438.953985452652 seconds.\n",
      "Processing file 1590000 after 15543.922289609909 seconds.\n",
      "Processing file 1600000 after 15648.145736694336 seconds.\n",
      "Processing file 1610000 after 15754.033022880554 seconds.\n",
      "Processing file 1620000 after 15857.332436323166 seconds.\n",
      "Processing file 1630000 after 15963.339404582977 seconds.\n",
      "Processing file 1640000 after 16066.05429649353 seconds.\n",
      "Processing file 1650000 after 16169.183161258698 seconds.\n",
      "Processing file 1660000 after 16272.513828992844 seconds.\n",
      "Processing file 1670000 after 16375.230451107025 seconds.\n",
      "Processing file 1680000 after 16479.136008262634 seconds.\n",
      "Processing file 1690000 after 16584.66839504242 seconds.\n",
      "Processing file 1700000 after 16689.901554584503 seconds.\n",
      "Processing file 1710000 after 16793.31164097786 seconds.\n",
      "Processing file 1720000 after 16898.55110001564 seconds.\n",
      "Processing file 1730000 after 17001.923911333084 seconds.\n",
      "Processing file 1740000 after 17106.527328252792 seconds.\n",
      "Processing file 1750000 after 17213.30798625946 seconds.\n",
      "Processing file 1760000 after 17322.89213681221 seconds.\n",
      "Processing file 1770000 after 17434.347948789597 seconds.\n",
      "Processing file 1780000 after 17549.043910741806 seconds.\n",
      "Processing file 1790000 after 17665.417410373688 seconds.\n",
      "Processing file 1800000 after 17781.846930265427 seconds.\n",
      "Processing file 1810000 after 17896.03358888626 seconds.\n",
      "Processing file 1820000 after 18013.35111784935 seconds.\n",
      "Processing file 1830000 after 18125.16538977623 seconds.\n",
      "Processing file 1840000 after 18241.823263645172 seconds.\n",
      "Processing file 1850000 after 18356.015969514847 seconds.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            sent_hede = score_article2(article.print_hede)\n",
    "            sent_lede = score_article2(article.lede)\n",
    "            sent_body = score_article2(article.paragraphs)\n",
    "            #sent_hede = 0\n",
    "            #sent_lede = 0\n",
    "            #sent_body = 0 # for now, let's not score the whole article, it takes so long\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 990857 train set files, 141551 dev set files, 283103 test set files.\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(len(meta_df) * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(len(meta_df) * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "drop_list = [] # holds the indices to drop\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        drop_list.append(row['filepath'])\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "\n",
    "filtered_df = train_df[~train_df.filepath.isin(drop_list)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# write (filtered) train log file\n",
    "filtered_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create comparative training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of filtered training set: 749744 . Size of comp training set: 749744\n"
     ]
    }
   ],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# load in filtered train log file\n",
    "train_filter_df = pd.read_csv(log_path+\"meta_train.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# randomly sample down to the size of the filtered data set to allow for apples-apples comparison\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED+1)\n",
    "full_set_size = len(train_df.index)\n",
    "filtered_set_size = len(train_filter_df.index)\n",
    "fraction = filtered_set_size / full_set_size # get fraction so we can grab a similar sample size \n",
    "train_filter_df = None # free up memory\n",
    "train_comp_df = train_df.sample(frac=fraction, axis=0, random_state=rando).reset_index(drop=True) # randomly sample\n",
    "\n",
    "# write (filtered) train log file\n",
    "train_comp_df.to_csv(path_or_buf=log_path+\"meta_comp.log\", index=False, header=True)\n",
    "print(\"Size of filtered training set:\", filtered_set_size,\". Size of comp training set:\", len(train_comp_df.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
