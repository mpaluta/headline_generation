{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1995/01/25/0739212.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.ipynb     A class for parsing the raw XML files\n",
    "#      |___ utilities.ipynb      Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=100,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return sentiments['sentiment'].mean()\n",
    "\n",
    "def score_article(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            sent_hede = score_article(article.print_hede)\n",
    "            sent_lede = score_article(article.lede)\n",
    "            sent_body = score_article(article.paragraphs)\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(all_files_count * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(all_files_count * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        train_df.drop(index, inplace=True)\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "        \n",
    "# write (filtered) train log file\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
