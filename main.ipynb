{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from statistics import mean \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1988/08/16/0171246.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.py        A class for parsing the raw XML files\n",
    "#      |___ utilities.py         Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ __init__.py          Required file for t2t\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=100,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster helper functions for sentiment analysis\n",
    "\n",
    "sentiment_dict = {} # stores tokens with their sentiment score for quick lookup\n",
    "\n",
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment2(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment2(words):\n",
    "    log_odds = [] # holds log odds\n",
    "    for word in words: # if we've seen this word before, look up the score in dictionary rather than model\n",
    "        if word in sentiment_dict:\n",
    "            log_odds.append(sentiment_dict[word])\n",
    "        else: # if we haven't seen word before, score it with model and add to dictionary for next time\n",
    "            score = vecs_to_sentiment2(embeddings.loc[[word]].dropna())[0]\n",
    "            sentiment_dict[word] = score\n",
    "            log_odds.append(score)\n",
    "    return log_odds\n",
    "\n",
    "def text_to_sentiment2(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment2(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return mean(sentiments)\n",
    "\n",
    "def score_article2(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment2(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10000 after 171.48450708389282 seconds.\n",
      "Processing file 20000 after 330.636182308197 seconds.\n",
      "Processing file 30000 after 467.1961224079132 seconds.\n",
      "Processing file 40000 after 597.5147225856781 seconds.\n",
      "Processing file 50000 after 720.1593947410583 seconds.\n",
      "Processing file 60000 after 841.3017289638519 seconds.\n",
      "Processing file 70000 after 968.048928976059 seconds.\n",
      "Processing file 80000 after 1086.9552960395813 seconds.\n",
      "Processing file 90000 after 1214.0839099884033 seconds.\n",
      "Processing file 100000 after 1339.090402841568 seconds.\n",
      "Processing file 110000 after 1453.904193162918 seconds.\n",
      "Processing file 120000 after 1578.2620248794556 seconds.\n",
      "Processing file 130000 after 1697.027447938919 seconds.\n",
      "Processing file 140000 after 1807.826308965683 seconds.\n",
      "Processing file 150000 after 1920.6806485652924 seconds.\n",
      "Processing file 160000 after 2028.6440165042877 seconds.\n",
      "Processing file 170000 after 2150.3191678524017 seconds.\n",
      "Processing file 180000 after 2264.1118342876434 seconds.\n",
      "Processing file 190000 after 2374.089067697525 seconds.\n",
      "Processing file 200000 after 2496.937079191208 seconds.\n",
      "Processing file 210000 after 2605.3464212417603 seconds.\n",
      "Processing file 220000 after 2716.503096342087 seconds.\n",
      "Processing file 230000 after 2830.82728767395 seconds.\n",
      "Processing file 240000 after 2936.2382571697235 seconds.\n",
      "Processing file 250000 after 3042.7903323173523 seconds.\n",
      "Processing file 260000 after 3146.0944695472717 seconds.\n",
      "Processing file 270000 after 3252.1299691200256 seconds.\n",
      "Processing file 280000 after 3365.431108236313 seconds.\n",
      "Processing file 290000 after 3467.8897652626038 seconds.\n",
      "Processing file 300000 after 3587.4019963741302 seconds.\n",
      "Processing file 310000 after 3690.936045408249 seconds.\n",
      "Processing file 320000 after 3797.1388778686523 seconds.\n",
      "Processing file 330000 after 3917.056746482849 seconds.\n",
      "Processing file 340000 after 4029.821171283722 seconds.\n",
      "Processing file 350000 after 4140.027901887894 seconds.\n",
      "Processing file 360000 after 4241.4545958042145 seconds.\n",
      "Processing file 370000 after 4343.623396873474 seconds.\n",
      "Processing file 380000 after 4453.109708070755 seconds.\n",
      "Processing file 390000 after 4541.945348739624 seconds.\n",
      "Processing file 400000 after 4643.815126657486 seconds.\n",
      "Processing file 410000 after 4740.937009334564 seconds.\n",
      "Processing file 420000 after 4839.168144226074 seconds.\n",
      "Processing file 430000 after 4931.377557992935 seconds.\n",
      "Processing file 440000 after 5024.516173839569 seconds.\n",
      "Processing file 450000 after 5112.867584228516 seconds.\n",
      "Processing file 460000 after 5200.365117073059 seconds.\n",
      "Processing file 470000 after 5293.516934633255 seconds.\n",
      "Processing file 480000 after 5386.544545650482 seconds.\n",
      "Processing file 490000 after 5484.301958084106 seconds.\n",
      "Processing file 500000 after 5580.130230665207 seconds.\n",
      "Processing file 510000 after 5676.612042665482 seconds.\n",
      "Processing file 520000 after 5771.501121997833 seconds.\n",
      "Processing file 530000 after 5862.2375519275665 seconds.\n",
      "Processing file 540000 after 5955.378745555878 seconds.\n",
      "Processing file 550000 after 6050.112725019455 seconds.\n",
      "Processing file 560000 after 6146.889363765717 seconds.\n",
      "Processing file 570000 after 6242.308443546295 seconds.\n",
      "Processing file 580000 after 6337.781657218933 seconds.\n",
      "Processing file 590000 after 6433.278073549271 seconds.\n",
      "Processing file 600000 after 6533.181858539581 seconds.\n",
      "Processing file 610000 after 6627.5738162994385 seconds.\n",
      "Processing file 620000 after 6726.33193731308 seconds.\n",
      "Processing file 630000 after 6821.607410430908 seconds.\n",
      "Processing file 640000 after 6918.692968130112 seconds.\n",
      "Processing file 650000 after 7014.846971750259 seconds.\n",
      "Processing file 660000 after 7112.149218320847 seconds.\n",
      "Processing file 670000 after 7209.405416727066 seconds.\n",
      "Processing file 680000 after 7308.811786651611 seconds.\n",
      "Processing file 690000 after 7404.46360874176 seconds.\n",
      "Processing file 700000 after 7504.0014543533325 seconds.\n",
      "Processing file 710000 after 7603.550077915192 seconds.\n",
      "Processing file 720000 after 7702.660078763962 seconds.\n",
      "Processing file 730000 after 7800.356554269791 seconds.\n",
      "Processing file 740000 after 7896.815175533295 seconds.\n",
      "Processing file 750000 after 7992.0990743637085 seconds.\n",
      "Processing file 760000 after 8082.74217748642 seconds.\n",
      "Processing file 770000 after 8173.084470748901 seconds.\n",
      "Processing file 780000 after 8260.830553770065 seconds.\n",
      "Processing file 790000 after 8349.895956516266 seconds.\n",
      "Processing file 800000 after 8437.25361251831 seconds.\n",
      "Processing file 810000 after 8522.882317781448 seconds.\n",
      "Processing file 820000 after 8608.493485689163 seconds.\n",
      "Processing file 830000 after 8697.116396188736 seconds.\n",
      "Processing file 840000 after 8785.645071029663 seconds.\n",
      "Processing file 850000 after 8873.39566874504 seconds.\n",
      "Processing file 860000 after 8961.048921108246 seconds.\n",
      "Processing file 870000 after 9048.907678604126 seconds.\n",
      "Processing file 880000 after 9149.700937986374 seconds.\n",
      "Processing file 890000 after 9249.02292561531 seconds.\n",
      "Processing file 900000 after 9345.258271694183 seconds.\n",
      "Processing file 910000 after 9428.68206691742 seconds.\n",
      "Processing file 920000 after 9511.901541948318 seconds.\n",
      "Processing file 930000 after 9594.07558131218 seconds.\n",
      "Processing file 940000 after 9678.325220823288 seconds.\n",
      "Processing file 950000 after 9762.393149137497 seconds.\n",
      "Processing file 960000 after 9843.547181367874 seconds.\n",
      "Processing file 970000 after 9929.253742218018 seconds.\n",
      "Processing file 980000 after 10015.106487035751 seconds.\n",
      "Processing file 990000 after 10101.485892295837 seconds.\n",
      "Processing file 1000000 after 10193.020903587341 seconds.\n",
      "Processing file 1010000 after 10282.034544944763 seconds.\n",
      "Processing file 1020000 after 10370.714415073395 seconds.\n",
      "Processing file 1030000 after 10459.791980743408 seconds.\n",
      "Processing file 1040000 after 10547.33961224556 seconds.\n",
      "Processing file 1050000 after 10637.592310905457 seconds.\n",
      "Processing file 1060000 after 10725.721575975418 seconds.\n",
      "Processing file 1070000 after 10813.554189443588 seconds.\n",
      "Processing file 1080000 after 10899.65134692192 seconds.\n",
      "Processing file 1090000 after 10985.186355352402 seconds.\n",
      "Processing file 1100000 after 11074.203916072845 seconds.\n",
      "Processing file 1110000 after 11166.800909042358 seconds.\n",
      "Processing file 1120000 after 11249.26012969017 seconds.\n",
      "Processing file 1130000 after 11334.959008455276 seconds.\n",
      "Processing file 1140000 after 11422.318624258041 seconds.\n",
      "Processing file 1150000 after 11512.962041378021 seconds.\n",
      "Processing file 1160000 after 11601.797389745712 seconds.\n",
      "Processing file 1170000 after 11691.587680339813 seconds.\n",
      "Processing file 1180000 after 11782.789020299911 seconds.\n",
      "Processing file 1190000 after 11874.195591926575 seconds.\n",
      "Processing file 1200000 after 11960.237540721893 seconds.\n",
      "Processing file 1210000 after 12047.71930027008 seconds.\n",
      "Processing file 1220000 after 12133.315486192703 seconds.\n",
      "Processing file 1230000 after 12216.382728338242 seconds.\n",
      "Processing file 1240000 after 12305.988508462906 seconds.\n",
      "Processing file 1250000 after 12395.477138996124 seconds.\n",
      "Processing file 1260000 after 12481.570250988007 seconds.\n",
      "Processing file 1270000 after 12566.568214893341 seconds.\n",
      "Processing file 1280000 after 12652.499583005905 seconds.\n",
      "Processing file 1290000 after 12737.976774930954 seconds.\n",
      "Processing file 1300000 after 12821.77977848053 seconds.\n",
      "Processing file 1310000 after 12906.937038898468 seconds.\n",
      "Processing file 1320000 after 12994.212540864944 seconds.\n",
      "Processing file 1330000 after 13082.181902885437 seconds.\n",
      "Processing file 1340000 after 13174.376638412476 seconds.\n",
      "Processing file 1350000 after 13266.114830255508 seconds.\n",
      "Processing file 1360000 after 13353.910432577133 seconds.\n",
      "Processing file 1370000 after 13441.973176956177 seconds.\n",
      "Processing file 1380000 after 13528.731932163239 seconds.\n",
      "Processing file 1390000 after 13617.545763492584 seconds.\n",
      "Processing file 1400000 after 13706.30704164505 seconds.\n",
      "Processing file 1410000 after 13795.787776708603 seconds.\n",
      "Processing file 1420000 after 13886.104781627655 seconds.\n",
      "Processing file 1430000 after 13977.923471450806 seconds.\n",
      "Processing file 1440000 after 14069.888673067093 seconds.\n",
      "Processing file 1450000 after 14160.039072751999 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1460000 after 14248.821406126022 seconds.\n",
      "Processing file 1470000 after 14338.436013936996 seconds.\n",
      "Processing file 1480000 after 14433.017293691635 seconds.\n",
      "Processing file 1490000 after 14523.18011379242 seconds.\n",
      "Processing file 1500000 after 14614.317165374756 seconds.\n",
      "Processing file 1510000 after 14702.07128739357 seconds.\n",
      "Processing file 1520000 after 14791.492219686508 seconds.\n",
      "Processing file 1530000 after 14881.803010225296 seconds.\n",
      "Processing file 1540000 after 14973.15970492363 seconds.\n",
      "Processing file 1550000 after 15059.810171365738 seconds.\n",
      "Processing file 1560000 after 15148.540688753128 seconds.\n",
      "Processing file 1570000 after 15237.742037534714 seconds.\n",
      "Processing file 1580000 after 15325.676841020584 seconds.\n",
      "Processing file 1590000 after 15416.39341545105 seconds.\n",
      "Processing file 1600000 after 15509.659691810608 seconds.\n",
      "Processing file 1610000 after 15601.028529882431 seconds.\n",
      "Processing file 1620000 after 15690.108407735825 seconds.\n",
      "Processing file 1630000 after 15780.816957235336 seconds.\n",
      "Processing file 1640000 after 15869.534616947174 seconds.\n",
      "Processing file 1650000 after 15959.280262947083 seconds.\n",
      "Processing file 1660000 after 16046.132756233215 seconds.\n",
      "Processing file 1670000 after 16134.281726360321 seconds.\n",
      "Processing file 1680000 after 16222.666934251785 seconds.\n",
      "Processing file 1690000 after 16315.549471855164 seconds.\n",
      "Processing file 1700000 after 16407.7417883873 seconds.\n",
      "Processing file 1710000 after 16497.8380651474 seconds.\n",
      "Processing file 1720000 after 16589.20256137848 seconds.\n",
      "Processing file 1730000 after 16678.309838056564 seconds.\n",
      "Processing file 1740000 after 16767.285113096237 seconds.\n",
      "Processing file 1750000 after 16856.418445825577 seconds.\n",
      "Processing file 1760000 after 16950.833872556686 seconds.\n",
      "Processing file 1770000 after 17045.063286542892 seconds.\n",
      "Processing file 1780000 after 17141.915774822235 seconds.\n",
      "Processing file 1790000 after 17240.882051706314 seconds.\n",
      "Processing file 1800000 after 17344.21739411354 seconds.\n",
      "Processing file 1810000 after 17444.06199669838 seconds.\n",
      "Processing file 1820000 after 17543.4399228096 seconds.\n",
      "Processing file 1830000 after 17639.61636710167 seconds.\n",
      "Processing file 1840000 after 17739.59773349762 seconds.\n",
      "Processing file 1850000 after 17837.169750213623 seconds.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            sent_hede = score_article2(article.print_hede)\n",
    "            sent_lede = score_article2(article.lede)\n",
    "            sent_body = score_article2(article.paragraphs)\n",
    "            #sent_hede = 0\n",
    "            #sent_lede = 0\n",
    "            #sent_body = 0 # for now, let's not score the whole article, it takes so long\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 990857 train set files, 141551 dev set files, 283103 test set files.\n",
      "WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(len(meta_df) * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(len(meta_df) * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 241113 training files due to sentiment.\n"
     ]
    }
   ],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "drop_list = [] # holds the indices to drop\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        drop_list.append(row['filepath'])\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "\n",
    "filtered_df = train_df[~train_df.filepath.isin(drop_list)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# write (filtered) train log file\n",
    "filtered_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
