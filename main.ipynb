{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from statistics import mean \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1988/08/16/0171246.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.py        A class for parsing the raw XML files\n",
    "#      |___ utilities.py         Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ __init__.py          Required file for t2t\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=100,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster helper functions for sentiment analysis\n",
    "\n",
    "sentiment_dict = {} # stores tokens with their sentiment score for quick lookup\n",
    "\n",
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment2(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment2(words):\n",
    "    log_odds = [] # holds log odds\n",
    "    for word in words: # if we've seen this word before, look up the score in dictionary rather than model\n",
    "        if word in sentiment_dict:\n",
    "            log_odds.append(sentiment_dict[word])\n",
    "        else: # if we haven't seen word before, score it with model and add to dictionary for next time\n",
    "            score = vecs_to_sentiment2(embeddings.loc[[word]].dropna())[0]\n",
    "            sentiment_dict[word] = score\n",
    "            log_odds.append(score)\n",
    "    return log_odds\n",
    "\n",
    "def text_to_sentiment2(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment2(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return mean(sentiments)\n",
    "\n",
    "def score_article2(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment2(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10000 after 195.16516280174255 seconds.\n",
      "Processing file 20000 after 325.459317445755 seconds.\n",
      "Processing file 30000 after 435.24300050735474 seconds.\n",
      "Processing file 40000 after 542.2706294059753 seconds.\n",
      "Processing file 50000 after 643.7244515419006 seconds.\n",
      "Processing file 60000 after 742.7937145233154 seconds.\n",
      "Processing file 70000 after 847.2087688446045 seconds.\n",
      "Processing file 80000 after 944.7176790237427 seconds.\n",
      "Processing file 90000 after 1049.1399374008179 seconds.\n",
      "Processing file 100000 after 1149.8335609436035 seconds.\n",
      "Processing file 110000 after 1242.0202612876892 seconds.\n",
      "Processing file 120000 after 1344.7671403884888 seconds.\n",
      "Processing file 130000 after 1442.6607344150543 seconds.\n",
      "Processing file 140000 after 1533.1849081516266 seconds.\n",
      "Processing file 150000 after 1624.388489484787 seconds.\n",
      "Processing file 160000 after 1711.272466659546 seconds.\n",
      "Processing file 170000 after 1811.0863943099976 seconds.\n",
      "Processing file 180000 after 1904.7016615867615 seconds.\n",
      "Processing file 190000 after 1993.561646938324 seconds.\n",
      "Processing file 200000 after 2093.5787942409515 seconds.\n",
      "Processing file 210000 after 2181.3778352737427 seconds.\n",
      "Processing file 220000 after 2272.5676572322845 seconds.\n",
      "Processing file 230000 after 2366.639594554901 seconds.\n",
      "Processing file 240000 after 2454.046540260315 seconds.\n",
      "Processing file 250000 after 2543.7197592258453 seconds.\n",
      "Processing file 260000 after 2630.357649087906 seconds.\n",
      "Processing file 270000 after 2719.023152112961 seconds.\n",
      "Processing file 280000 after 2815.3406121730804 seconds.\n",
      "Processing file 290000 after 2902.9124755859375 seconds.\n",
      "Processing file 300000 after 3004.5949008464813 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f27c06142014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# calc sentiment data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msent_hede\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_article2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_hede\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msent_lede\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_article2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlede\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0msent_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_article2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#sent_hede = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0103a009c940>\u001b[0m in \u001b[0;36mscore_article2\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mnum_sentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext_to_sentiment2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0103a009c940>\u001b[0m in \u001b[0;36mtext_to_sentiment2\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_sentiment2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_sentiment2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# literal replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            sent_hede = score_article2(article.print_hede)\n",
    "            sent_lede = score_article2(article.lede)\n",
    "            sent_body = score_article2(article.paragraphs)\n",
    "            #sent_hede = 0\n",
    "            #sent_lede = 0\n",
    "            #sent_body = 0 # for now, let's not score the whole article, it takes so long\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(len(meta_df) * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(len(meta_df) * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "drop_list = [] # holds the indices to drop\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        drop_list.append(row['filepath'])\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "\n",
    "filtered_df = train_df[~train_df.filepath.isin(drop_list)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# write (filtered) train log file\n",
    "filtered_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create comparative training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of filtered training set: 749744 . Size of comp training set: 749744\n"
     ]
    }
   ],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# load in filtered train log file\n",
    "train_filter_df = pd.read_csv(log_path+\"meta_train.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# randomly sample down to the size of the filtered data set to allow for apples-apples comparison\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED+1)\n",
    "full_set_size = len(train_df.index)\n",
    "filtered_set_size = len(train_filter_df.index)\n",
    "fraction = filtered_set_size / full_set_size # get fraction so we can grab a similar sample size \n",
    "train_filter_df = None # free up memory\n",
    "train_comp_df = train_df.sample(frac=fraction, axis=0, random_state=rando).reset_index(drop=True) # randomly sample\n",
    "\n",
    "# write (filtered) train log file\n",
    "train_comp_df.to_csv(path_or_buf=log_path+\"meta_comp.log\", index=False, header=True)\n",
    "print(\"Size of filtered training set:\", filtered_set_size,\". Size of comp training set:\", len(train_comp_df.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
