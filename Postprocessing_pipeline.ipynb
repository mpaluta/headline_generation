{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "import pandas as pd\n",
    "from statistics import mean \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath vars -- COMMENT IN/OUT RF VS SF DEPENDING ON FILE INGESTING\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "postp_path = './postprocess/' # points to folder containing logs and files for evaluating the models\n",
    "test_log = 'meta_test.log' # points to the test file log\n",
    "decoder_path = './decoder/' # points to folder with decoder output files\n",
    "decoder_output = 'decoder_rf.txt' # points to file with decoder headlines from randomly filtered model\n",
    "#decoder_output = 'decoder_sf.txt' # points to file with decoder headlines from sentiment filtered model\n",
    "postp_sentiments = 'sents_rf.txt' # points to file with sentiment scores from randomly filtered model\n",
    "#postp_sentiments = 'sents_sf.txt' # points to file with sentiment scores from sentiment filtered model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce files for decoder ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.528217631618259"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get baseline word count from train data\n",
    "df_train = pd.read_csv(os.path.join(log_path,\"meta_train.log\"))\n",
    "hede_avg = math.ceil(df_train.hede_size.mean())\n",
    "df_train.hede_size.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>hede_size</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>section</th>\n",
       "      <th>sent_hede</th>\n",
       "      <th>sent_lede</th>\n",
       "      <th>sent_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003/08/22/1513837.xml</td>\n",
       "      <td>11</td>\n",
       "      <td>832</td>\n",
       "      <td>['bombs and explosives', 'international relati...</td>\n",
       "      <td>-1.066886</td>\n",
       "      <td>0.401246</td>\n",
       "      <td>0.136112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004/06/19/1590476.xml</td>\n",
       "      <td>8</td>\n",
       "      <td>636</td>\n",
       "      <td>['education and schools', 'grading of students...</td>\n",
       "      <td>-0.553291</td>\n",
       "      <td>1.198004</td>\n",
       "      <td>1.375394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002/10/08/1430194.xml</td>\n",
       "      <td>6</td>\n",
       "      <td>102</td>\n",
       "      <td>['news and news media', 'public opinion', 'tel...</td>\n",
       "      <td>2.452781</td>\n",
       "      <td>0.980704</td>\n",
       "      <td>0.255741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995/08/11/0781753.xml</td>\n",
       "      <td>6</td>\n",
       "      <td>514</td>\n",
       "      <td>['blood', 'transfusions']</td>\n",
       "      <td>-0.997738</td>\n",
       "      <td>-0.442929</td>\n",
       "      <td>0.241921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991/06/30/0456560.xml</td>\n",
       "      <td>4</td>\n",
       "      <td>337</td>\n",
       "      <td>['music']</td>\n",
       "      <td>4.567766</td>\n",
       "      <td>0.980704</td>\n",
       "      <td>1.183736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 filepath  hede_size  wordcount  \\\n",
       "0  2003/08/22/1513837.xml         11        832   \n",
       "1  2004/06/19/1590476.xml          8        636   \n",
       "2  2002/10/08/1430194.xml          6        102   \n",
       "3  1995/08/11/0781753.xml          6        514   \n",
       "4  1991/06/30/0456560.xml          4        337   \n",
       "\n",
       "                                             section  sent_hede  sent_lede  \\\n",
       "0  ['bombs and explosives', 'international relati...  -1.066886   0.401246   \n",
       "1  ['education and schools', 'grading of students...  -0.553291   1.198004   \n",
       "2  ['news and news media', 'public opinion', 'tel...   2.452781   0.980704   \n",
       "3                          ['blood', 'transfusions']  -0.997738  -0.442929   \n",
       "4                                          ['music']   4.567766   0.980704   \n",
       "\n",
       "   sent_body  \n",
       "0   0.136112  \n",
       "1   1.375394  \n",
       "2   0.255741  \n",
       "3   0.241921  \n",
       "4   1.183736  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(log_path,test_log))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAF_LIMIT = 3\n",
    "\n",
    "with open(os.path.join(\"postprocess\", \"headlines.txt\"), \"w+\") as headlines, \\\n",
    "    open(os.path.join(\"postprocess\", \"bodies.txt\"), \"w+\") as bodies, \\\n",
    "    open(os.path.join(\"postprocess\", \"baseline.txt\"), \"w+\") as baseline:\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        article = NYTArticle.from_file(os.path.join(\"data\",\"nyt\",row.filepath))\n",
    "        \n",
    "        headlines.write(article.print_hede[0]+\"\\n\")\n",
    "        bodies.write(\" \".join(article.paragraphs[:GRAF_LIMIT])+\"\\n\")\n",
    "        try:\n",
    "            baseline.write(\" \".join(\" \".join(article.paragraphs).split()[:hede_avg])+\"\\n\")\n",
    "        except:\n",
    "            baseline.write(\" \".join(article.paragraphs))\n",
    "#         print(article.print_hede[0])\n",
    "#         print(\" \".join(article.paragraphs[:GRAF_LIMIT]))\n",
    "#         print(\" \".join(\" \".join(article.paragraphs).split()[:hede_avg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster helper functions for sentiment analysis\n",
    "\n",
    "sentiment_dict = {} # stores tokens with their sentiment score for quick lookup\n",
    "\n",
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment2(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment2(words):\n",
    "    log_odds = [] # holds log odds\n",
    "    for word in words: # if we've seen this word before, look up the score in dictionary rather than model\n",
    "        if word in sentiment_dict:\n",
    "            log_odds.append(sentiment_dict[word])\n",
    "        else: # if we haven't seen word before, score it with model and add to dictionary for next time\n",
    "            score = vecs_to_sentiment2(embeddings.loc[[word]].dropna())[0]\n",
    "            sentiment_dict[word] = score\n",
    "            log_odds.append(score)\n",
    "    return log_odds\n",
    "\n",
    "def text_to_sentiment2(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment2(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return mean(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score decoder output headlines for sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with preprocessed sentiment scores into a pd.DataFrame\n",
    "scores_df = pd.read_csv(postp_path+test_log, sep=\",\", header=0,  \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# read in the headlines from the decoder output file\n",
    "with open(decoder_path+decoder_output) as f:\n",
    "    headlines = [headlines.rstrip('\\n') for headlines in f]\n",
    "\n",
    "genhede_sent = [text_to_sentiment2(hede) for hede in headlines] # calc sentiment for decoder headlines\n",
    "scores_df['sent_decoder'] = genhede_sent # add decoder headline sentiment to dataframe\n",
    "scores_df.to_csv(path_or_buf=postp_path+postp_sentiments, index=False, header=True) # save scores to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sentiment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1_sentiment\n",
    "# This is percentage of the generated headlines have sentiment polarity (+/-) that matches lede sentiment polarity.\n",
    "# This can also be thought of as an F1-score, calculated as: \n",
    "# F1 = 2 * (Precision*Recall) / (Precision+Recall)\n",
    "# Sentiment difference measures the average test headline label sentiment minus average decoder generated headline sentiment.\n",
    "\n",
    "# \n",
    "TP = 0 # true positive count\n",
    "TN = 0 # true negative count\n",
    "FP = 0 # false positive count\n",
    "FN = 0 # false negative count\n",
    "match = 0\n",
    "total = len(scores_df.sent_decoder)\n",
    "\n",
    "for index, row in scores_df.iterrows():\n",
    "    \n",
    "    # f1-score calcs\n",
    "    if row.sent_hede >=0 and row.sent_decoder >=0: TP += 1\n",
    "    elif row.sent_hede >= 0 and row.sent_decoder <0: FN += 1\n",
    "    elif row.sent_hede <0 and row.sent_decoder >=0: FP += 1\n",
    "    else: TN += 1\n",
    "    \n",
    "    # confirm\n",
    "    if row.sent_hede >= 0 and row.sent_decoder >=0: match +=1\n",
    "    if row.sent_hede <0 and row.sent_decoder <0: match +=1\n",
    "    \n",
    "average_hede = mean(scores_df.sent_hede)\n",
    "average_decoder = mean(scores_df.sent_decoder)\n",
    "average_difference = average_hede - average_decoder\n",
    "\n",
    "precision = TP / (TP+FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1-score = 2 * (precision*recall) / (precision+recall)\n",
    "confirm = match/total\n",
    "\n",
    "print(\"The f1-score is\", f1-score,\"Confirming:\", confirm)\n",
    "print(\"The average test headline score is\", average_hede,\"The average decoder headline score is\", average_decoder,\"Sentiment difference is\", average_difference)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
