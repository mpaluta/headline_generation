{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base prefix 'c:\\\\users\\\\benar\\\\anaconda3'\n",
      "  No LICENSE.txt / LICENSE found in source\n",
      "New python executable in C:\\Users\\benar\\Desktop\\project_data\\bertenv\\Scripts\\python.exe\n",
      "Installing setuptools, pip, wheel...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "!virtualenv bertenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "import pandas as pd\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pathways\n",
    "train_paths = data_setup.get_paths('train')\n",
    "article = NYTArticle.from_file(os.path.join('data', train_paths[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set destination folder and make folders if needed\n",
    "dest_folder = 'sample/train/'\n",
    "if not os.path.exists(dest_folder):\n",
    "    os.makedirs(dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample/train/739212\n"
     ]
    }
   ],
   "source": [
    "# attempt a single file output\n",
    "print(\"Writing\", dest_folder+article.docid)\n",
    "article.simple_csv_output(dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample/train/1397928\n",
      "Writing sample/train/1609775\n",
      "Writing sample/train/297100\n",
      "Writing sample/train/448007\n",
      "Writing sample/train/1463610\n",
      "Writing sample/train/590096\n",
      "Writing sample/train/762936\n",
      "Writing sample/train/1570877\n",
      "Writing sample/train/1725268\n",
      "Writing sample/train/1647864\n",
      "Writing sample/train/1251662\n",
      "Writing sample/train/551924\n",
      "Writing sample/train/74723\n",
      "Writing sample/train/1535917\n",
      "Writing sample/train/744457\n",
      "Writing sample/train/97129\n",
      "Writing sample/train/899133\n",
      "Writing sample/train/791668\n",
      "Writing sample/train/779448\n",
      "Writing sample/train/1287650\n",
      "Writing sample/train/927546\n",
      "Writing sample/train/1587613\n",
      "Writing sample/train/638654\n",
      "Writing sample/train/870914\n",
      "Writing sample/train/800527\n",
      "Writing sample/train/1584905\n",
      "Writing sample/train/1287373\n",
      "Writing sample/train/1337862\n",
      "Writing sample/train/1646102\n",
      "Writing sample/train/1363692\n",
      "Writing sample/train/50986\n",
      "Writing sample/train/1481311\n",
      "Writing sample/train/1265869\n",
      "Writing sample/train/199802\n",
      "Writing sample/train/1000735\n",
      "Writing sample/train/1537677\n",
      "Writing sample/train/700412\n",
      "Writing sample/train/1043116\n",
      "Writing sample/train/1509284\n",
      "Writing sample/train/1562666\n"
     ]
    }
   ],
   "source": [
    "# attempt 50 article write minus filtered out\n",
    "for path in train_paths[:50]:\n",
    "    article = NYTArticle.from_file(os.path.join('data',path[0]))\n",
    "    if article.pass_filters():\n",
    "        print(\"Writing\", dest_folder+article.docid)\n",
    "        article.simple_csv_output(dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened 10000 articles. Output 8341 articles.\n",
      "\n",
      "... processed 10000 files in 4.4035398960113525 seconds\n"
     ]
    }
   ],
   "source": [
    "# write all filtered articles in train set\n",
    "opened_article = 0\n",
    "written_article = 0\n",
    "start = time.time()\n",
    "for path in train_paths[:10000]:\n",
    "    opened_article += 1\n",
    "    article = NYTArticle.from_file(os.path.join('data',path[0]))\n",
    "    if article.pass_filters():\n",
    "        written_article += 1\n",
    "        article.simple_csv_output(dest_folder)\n",
    "    if opened_article % 10000 == 0:\n",
    "        print(\"Opened\", opened_article,\"articles. Output\", written_article,\"articles.\")\n",
    "print(f\"\\n... processed {opened_article} files in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Killer, Yes, but She's a Good Old Girl</td>\n",
       "      <td>Aileen Wuornos was big news 10 years ago. At a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How High's The Bidding, Mama?</td>\n",
       "      <td>THE blue Rolls Royce sat at the York Street cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pacificorp reports earnings for Qtr to Sept 30</td>\n",
       "      <td>LEAD:\\n*3*** COMPANY REPORTS **\\n*3* Pacificor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stewart Set to Return</td>\n",
       "      <td>There is bad news for Oakland's competition in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taxing Online Sales</td>\n",
       "      <td>To the Editor:\\nRe ''So Many Online Sales, So ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         headline  \\\n",
       "0        A Killer, Yes, but She's a Good Old Girl   \n",
       "1                   How High's The Bidding, Mama?   \n",
       "2  Pacificorp reports earnings for Qtr to Sept 30   \n",
       "3                           Stewart Set to Return   \n",
       "4                             Taxing Online Sales   \n",
       "\n",
       "                                                body  \n",
       "0  Aileen Wuornos was big news 10 years ago. At a...  \n",
       "1  THE blue Rolls Royce sat at the York Street cu...  \n",
       "2  LEAD:\\n*3*** COMPANY REPORTS **\\n*3* Pacificor...  \n",
       "3  There is bad news for Oakland's competition in...  \n",
       "4  To the Editor:\\nRe ''So Many Online Sales, So ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe with articles\n",
    "sample = 1000\n",
    "\n",
    "headlines = []\n",
    "bodies = []\n",
    "\n",
    "for path in train_paths[:sample]:\n",
    "    article = NYTArticle.from_file(os.path.join('data',path[0]))\n",
    "    if article.pass_filters():\n",
    "        article.simple_csv_output(dest_folder)\n",
    "        headlines.append(article.print_hede[0])\n",
    "        # join paragraphs for now, may want to do something else later\n",
    "        bodies.append('\\n'.join(article.paragraphs))\n",
    "\n",
    "train_df = pd.DataFrame(data={'headline':headlines, 'body':bodies})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps\n",
    "!pip install -q -U tensor2tensor\n",
    "!pip install -q tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we need.\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "# Enable TF Eager execution\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "# Setup some directories\n",
    "data_dir = os.path.expanduser(\"./t2t/data\")\n",
    "tmp_dir = os.path.expanduser(\"./t2t/tmp\")\n",
    "train_dir = os.path.expanduser(\"./t2t/train\")\n",
    "checkpoint_dir = os.path.expanduser(\"./t2t/checkpoints\")\n",
    "tf.gfile.MakeDirs(data_dir)\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "tf.gfile.MakeDirs(train_dir)\n",
    "tf.gfile.MakeDirs(checkpoint_dir)\n",
    "gs_data_dir = \"gs://tensor2tensor-data/\"\n",
    "gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems.available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://tensor2tensor-data/vocab.summarize_cnn_dailymail32k.32768.subwords...\n",
      "/ [1 files][293.7 KiB/293.7 KiB]                                                \n",
      "Operation completed over 1 objects/293.7 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Fetch the problem\n",
    "sumCNN_problem = problems.problem(\"summarize_cnn_dailymail32k\")\n",
    "\n",
    "# Copy the vocab file locally so we can encode inputs and decode model outputs\n",
    "# All vocabs are stored on GCS\n",
    "# vocab_name = \"vocab.translate_ende_wmt32k.32768.subwords\"\n",
    "vocab_name = \"vocab.summarize_cnn_dailymail32k.32768.subwords\"\n",
    "vocab_file = os.path.join(gs_data_dir, vocab_name)\n",
    "!gsutil cp {vocab_file} {data_dir}\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = sumCNN_problem.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and view the data\n",
    "# This cell is commented out because WMT data generation can take hours\n",
    "\n",
    "# sumCNN_problem.generate_data(data_dir, tmp_dir)\n",
    "# example = tfe.Iterator(sumCNN_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
    "# inputs = [int(x) for x in example[\"inputs\"].numpy()] # Cast to ints.\n",
    "# targets = [int(x) for x in example[\"targets\"].numpy()] # Cast to ints.\n",
    "\n",
    "# # Example inputs as int-tensor.\n",
    "# print(\"Inputs, encoded:\")\n",
    "# print(inputs)\n",
    "# print(\"Inputs, decoded:\")\n",
    "# # Example inputs as a sentence.\n",
    "# print(decode(inputs))\n",
    "# # Example targets as int-tensor.\n",
    "# print(\"Targets, encoded:\")\n",
    "# print(targets)\n",
    "# # Example targets as a sentence.\n",
    "# print(\"Targets, decoded:\")\n",
    "# print(decode(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many models available in Tensor2Tensor\n",
    "# NOTE THAT THERE IS A UNIVERSAL TRANSFORMER IN HERE\n",
    "# registry.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hparams and the model\n",
    "model_name = \"transformer\"\n",
    "hparams_set = \"transformer_prepend\"\n",
    "\n",
    "hparams = trainer_lib.create_hparams(hparams_set, data_dir=data_dir, problem_name=\"summarize_cnn_dailymail32k\")\n",
    "\n",
    "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
    "# Layer and so subsequent instantiations will have different variable scopes\n",
    "# that will not match the checkpoint.\n",
    "sumCNN_model = registry.model(model_name)(hparams, Modes.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the pretrained checkpoint locally\n",
    "# ckpt_name = \"transformer_ende_test\"\n",
    "# gs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\n",
    "# !gsutil -q cp -R {gs_ckpt} {checkpoint_dir}\n",
    "# ckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\n",
    "# ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What checkpoints are available\n",
    "# gs://tensor2tensor-data/\n",
    "#     gs_data_dir\n",
    "# !gsutil ls {gs_ckpt_dir}\n",
    "# !gsutil ls {gs_data_dir}\n",
    "# !gsutil -q cp -R {'gs://tensor2tensor-data/vocab.cnndailymail.32768'} {checkpoint_dir}\n",
    "# !gsutil -q cp -R {'gs://tensor2tensor-data/vocab.summarize_cnn_dailymail32k.32768.subwords'} {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for the training loop\n",
    "\n",
    "# In Eager mode, opt.minimize must be passed a loss function wrapped with\n",
    "# implicit_value_and_gradients\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = sumCNN_model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "# Setup the training data\n",
    "BATCH_SIZE = 1\n",
    "sumCNN_problem = problems.problem(\"summarize_cnn_dailymail32k\")\n",
    "sumCNN_dataset = sumCNN_problem.dataset(Modes.TRAIN, data_dir)\n",
    "sumCNN_dataset = sumCNN_dataset.repeat(None).batch(BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 9.605\n",
      "Step: 1, Loss: 9.355\n",
      "Step: 2, Loss: 8.356\n",
      "Step: 3, Loss: 8.711\n",
      "Step: 4, Loss: 8.045\n",
      "Step: 5, Loss: 7.883\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "NUM_STEPS = 5\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(sumCNN_dataset)):\n",
    "#   print(example[\"targets\"])\n",
    "#   example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "#   if count % 50 == 0:\n",
    "#     print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = \"\"\"LEAD: New estimates by the Congressional Budget Office show that the cost of Government benefit programs is increasing much faster than had been expected.', 'New estimates by the Congressional Budget Office show that the cost of Government benefit programs is increasing much faster than had been expected.', 'The increase appears to result in part from decisions by state officials to expand social welfare programs such as Medicaid, which provides health care to the poor and is financed jointly by the Federal Government and the states.', 'In the Medicaid program, states set eligibility criteria and define the scope of benefits within guidelines set by Federal law. In the last year many states have expanded Medicaid eligibility or increased benefits to provide more assistance to pregnant women and children from low-income families.', 'In each state, the Federal Government pays at least half the costs of Medicaid and Aid to Families with Dependent Children, the main Federal-state welfare program. Giant Share of U.S. Spending', 'Benefit programs account for nearly half of all Federal spending. In August the budget office estimated that the cost of these programs would rise from $467 billion in the current fiscal year, 1987, to $499 billion in 1988 and eventually to $593 billion in 1991.', 'Aides to Rudolph G. Penner, director of the Congressional Budget Office, said today that the new estimates were $10 billion to $15 billion higher for 1987 and for 1988, and more than $20 billion higher for 1991.', 'These figures suggest that it will be difficult for Congress to meet the deficit targets set in the budget-balancing law signed by President Reagan in December 1985. The targets are $144 billion for 1987 and $108 billion for 1988. Under the law, the budget is supposed to be balanced by 1991.', \"\"Mr. Reagan plans to submit his 1988 budget to Congress on Monday. His budget director, James C. Miller 3d, has said that the President's budget would show a deficit of no more than $108 billion in 1988. Estimates of the cost of benefit programs have not yet been made public, and the budget does not contain a figure combining all the benefit programs.\"\", 'The latest estimates by the Congressional Budget Office mean that Democrats will be under new pressure to restrain the growth of domestic programs that they might otherwise want to expand. Democrats in the Senate and the House have begun drafting legislation to expand welfare, Medicare, the Federal health insurance program for the elderly and disabled, and Medicaid.', 'Under the major benefit programs, such as Social Security, student loans and unemployment compensation, people are entitled to benefits if they meet certain eligibility requirements set by law. In some of the programs, there is a test of financial need. But in others, people can obtain benefits regardless of their income or asssets.', 'Several of the programs provide assistance to farmers to support the prices of agricultural commodities such as corn, wheat, rice and cotton. Aides to Mr. Penner said the projected costs of these programs had risen substantially above the levels estimated in August: $19.7 billion for 1987 and $21.6 billion for 1988. But they said the new figures would not be disclosed for at least a week. Change in Revenue Estimates', \"\"Administration officials said President Reagan's budget would show revenue dramatically different from that estimated by Congress when it approved a major tax bill in September.\"\", 'At that time, the Congressional Joint Committee on Taxation compared the new measure with prior law and concluded that it would generate $11 billion more in 1987, but $17 billion less in 1988.', \"\"But Mr. Reagan's budget, using estimates developed by the Treasury Department, says the new law will produce a bigger gain in 1987, $18 billion, and no loss at all in 1988.\"\", 'David H. Brockway, chief of staff for the joint committee, said the panel had not changed its estimates.', \"\"The Treasury's figures have political implications because they would make it easier for the President and Congress to reduce the deficit in 1987 and 1988. But in later years the Treasury's estimates would have a different effect, because they show smaller gains in revenue, or more of a loss, than the joint committee's estimates.\"\", 'Most state income-tax laws are linked to Federal tax law in some way. Thirty-five states are expected to gain revenue if they make no change in their own laws. Some of the states plan to use part of that money to expand social welfare benefits.', 'Such changes could require a further increase in Federal spending because the Federal Government shares the cost of several major social welfare programs. The Federal contribution ranges from 50 percent to 78 percent of the cost of benefits under Medicaid and Aid to Families with Dependent Children. Increase for Science Foundation', 'While proposing cuts in many domestic programs, Administration officials said Mr. Reagan would seek an increase of more than 15 percent in the budget of the National Science Foundation, which supports research in science and engineering. The agency has a budget of $1.6 billion this year.', \"\"The proposed increase is part of Mr. Reagan's overall effort to ''keep America competitive'' by maintaining its technological edge over other countries.\"\", 'But budget officials said Mr. Reagan would ask Congress to slash the budget of the Corporation for Public Broadcasting, which supports public radio and television stations around the country. Congress has already voted to give the corporation $214 million in 1988, $228 million in 1989 and $254 million 1990.', \"\"Mr. Reagan wants to cut the corporation's budget to $170 million in 1988, $140 million in 1989 and $132 million in 1990, according to confidential budget documents.\"\"]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: Hello, my name is Ben. And my partner on this project is Mark.\n",
      "Outputs: Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer Singer\n"
     ]
    }
   ],
   "source": [
    "# Restore and translate!\n",
    "def translate(inputs):\n",
    "  encoded_inputs = encode(inputs)\n",
    "#   with tfe.restore_variables_on_create(ckpt_path):\n",
    "#     model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n",
    "  model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n",
    "  return decode(model_output)\n",
    "\n",
    "inputs = \"Hello, my name is Ben. And my partner on this project is Mark.\"\n",
    "outputs = translate(inputs)\n",
    "\n",
    "print(\"Inputs: %s\" % inputs)\n",
    "print(\"Outputs: %s\" % outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
