{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from statistics import mean \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1994/02/02/0666100.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.py        A class for parsing the raw XML files\n",
    "#      |___ utilities.py         Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ __init__.py          Required file for t2t\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/home/arnoldyb/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=100,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return sentiments['sentiment'].mean()\n",
    "\n",
    "def score_article(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster helper functions for sentiment analysis\n",
    "\n",
    "sentiment_dict = {} # stores tokens with their sentiment score for quick lookup\n",
    "\n",
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment2(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment2(words):\n",
    "    log_odds = [] # holds log odds\n",
    "    for word in words: # if we've seen this word before, look up the score in dictionary rather than model\n",
    "        if word in sentiment_dict:\n",
    "            log_odds.append(sentiment_dict[word])\n",
    "        else: # if we haven't seen word before, score it with model and add to dictionary for next time\n",
    "            score = vecs_to_sentiment2(embeddings.loc[[word]].dropna())[0]\n",
    "            sentiment_dict[word] = score\n",
    "            log_odds.append(score)\n",
    "    return log_odds\n",
    "\n",
    "def text_to_sentiment2(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment2(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return mean(sentiments)\n",
    "\n",
    "def score_article2(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment2(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10000 after 236.83385348320007 seconds.\n",
      "Processing file 20000 after 398.7414107322693 seconds.\n",
      "Processing file 30000 after 536.11501121521 seconds.\n",
      "Processing file 40000 after 668.5217697620392 seconds.\n",
      "Processing file 50000 after 795.3351137638092 seconds.\n",
      "Processing file 60000 after 921.0994596481323 seconds.\n",
      "Processing file 70000 after 1053.678061723709 seconds.\n",
      "Processing file 80000 after 1178.3731000423431 seconds.\n",
      "Processing file 90000 after 1308.9233248233795 seconds.\n",
      "Processing file 100000 after 1436.3585834503174 seconds.\n",
      "Processing file 110000 after 1555.3802707195282 seconds.\n",
      "Processing file 120000 after 1685.6217584609985 seconds.\n",
      "Processing file 130000 after 1809.9616570472717 seconds.\n",
      "Processing file 140000 after 1925.9131734371185 seconds.\n",
      "Processing file 150000 after 2044.1033298969269 seconds.\n",
      "Processing file 160000 after 2155.9881298542023 seconds.\n",
      "Processing file 170000 after 2279.3760616779327 seconds.\n",
      "Processing file 180000 after 2395.99134349823 seconds.\n",
      "Processing file 190000 after 2508.759083509445 seconds.\n",
      "Processing file 200000 after 2635.803338766098 seconds.\n",
      "Processing file 210000 after 2749.545935153961 seconds.\n",
      "Processing file 220000 after 2864.7369656562805 seconds.\n",
      "Processing file 230000 after 2986.1097099781036 seconds.\n",
      "Processing file 240000 after 3097.239135980606 seconds.\n",
      "Processing file 250000 after 3212.0001537799835 seconds.\n",
      "Processing file 260000 after 3324.282856464386 seconds.\n",
      "Processing file 270000 after 3438.1060814857483 seconds.\n",
      "Processing file 280000 after 3557.6613392829895 seconds.\n",
      "Processing file 290000 after 3665.4133903980255 seconds.\n",
      "Processing file 300000 after 3790.9610838890076 seconds.\n",
      "Processing file 310000 after 3900.564865589142 seconds.\n",
      "Processing file 320000 after 4010.752252101898 seconds.\n",
      "Processing file 330000 after 4136.463628292084 seconds.\n",
      "Processing file 340000 after 4255.105700492859 seconds.\n",
      "Processing file 350000 after 4372.937846422195 seconds.\n",
      "Processing file 360000 after 4482.0934591293335 seconds.\n",
      "Processing file 370000 after 4590.29887342453 seconds.\n",
      "Processing file 380000 after 4706.734155416489 seconds.\n",
      "Processing file 390000 after 4802.482929229736 seconds.\n",
      "Processing file 400000 after 4910.6236979961395 seconds.\n",
      "Processing file 410000 after 5014.187209606171 seconds.\n",
      "Processing file 420000 after 5117.1867570877075 seconds.\n",
      "Processing file 430000 after 5212.797808170319 seconds.\n",
      "Processing file 440000 after 5307.966561079025 seconds.\n",
      "Processing file 450000 after 5398.447101593018 seconds.\n",
      "Processing file 460000 after 5489.170656681061 seconds.\n",
      "Processing file 470000 after 5585.6738567352295 seconds.\n",
      "Processing file 480000 after 5681.970979213715 seconds.\n",
      "Processing file 490000 after 5782.470713376999 seconds.\n",
      "Processing file 500000 after 5880.426789999008 seconds.\n",
      "Processing file 510000 after 5978.543328046799 seconds.\n",
      "Processing file 520000 after 6074.852097272873 seconds.\n",
      "Processing file 530000 after 6166.685399055481 seconds.\n",
      "Processing file 540000 after 6261.611884832382 seconds.\n",
      "Processing file 550000 after 6358.5555164813995 seconds.\n",
      "Processing file 560000 after 6456.289252281189 seconds.\n",
      "Processing file 570000 after 6552.938720464706 seconds.\n",
      "Processing file 580000 after 6652.316600799561 seconds.\n",
      "Processing file 590000 after 6749.164867639542 seconds.\n",
      "Processing file 600000 after 6848.632628917694 seconds.\n",
      "Processing file 610000 after 6944.487329721451 seconds.\n",
      "Processing file 620000 after 7043.6120483875275 seconds.\n",
      "Processing file 630000 after 7138.87487745285 seconds.\n",
      "Processing file 640000 after 7234.377009868622 seconds.\n",
      "Processing file 650000 after 7329.056433200836 seconds.\n",
      "Processing file 660000 after 7428.26060295105 seconds.\n",
      "Processing file 670000 after 7529.345352649689 seconds.\n",
      "Processing file 680000 after 7628.049670934677 seconds.\n",
      "Processing file 690000 after 7719.665616512299 seconds.\n",
      "Processing file 700000 after 7816.238761425018 seconds.\n",
      "Processing file 710000 after 7910.543687343597 seconds.\n",
      "Processing file 720000 after 8007.466372251511 seconds.\n",
      "Processing file 730000 after 8103.752613067627 seconds.\n",
      "Processing file 740000 after 8198.66298031807 seconds.\n",
      "Processing file 750000 after 8294.1542699337 seconds.\n",
      "Processing file 760000 after 8384.344054222107 seconds.\n",
      "Processing file 770000 after 8476.239735603333 seconds.\n",
      "Processing file 780000 after 8562.584497451782 seconds.\n",
      "Processing file 790000 after 8650.654856443405 seconds.\n",
      "Processing file 800000 after 8737.543664455414 seconds.\n",
      "Processing file 810000 after 8822.709173202515 seconds.\n",
      "Processing file 820000 after 8907.757989645004 seconds.\n",
      "Processing file 830000 after 8996.41177368164 seconds.\n",
      "Processing file 840000 after 9085.295125484467 seconds.\n",
      "Processing file 850000 after 9172.790173768997 seconds.\n",
      "Processing file 860000 after 9259.899643421173 seconds.\n",
      "Processing file 870000 after 9350.55080628395 seconds.\n",
      "Processing file 880000 after 9455.045595169067 seconds.\n",
      "Processing file 890000 after 9558.047217130661 seconds.\n",
      "Processing file 900000 after 9658.453720092773 seconds.\n",
      "Processing file 910000 after 9751.714664936066 seconds.\n",
      "Processing file 920000 after 9845.361709833145 seconds.\n",
      "Processing file 930000 after 9937.46362376213 seconds.\n",
      "Processing file 940000 after 10030.364612817764 seconds.\n",
      "Processing file 950000 after 10123.150255918503 seconds.\n",
      "Processing file 960000 after 10212.821732521057 seconds.\n",
      "Processing file 970000 after 10304.361730337143 seconds.\n",
      "Processing file 980000 after 10396.06460404396 seconds.\n",
      "Processing file 990000 after 10486.732549905777 seconds.\n",
      "Processing file 1000000 after 10580.472153425217 seconds.\n",
      "Processing file 1010000 after 10673.240609645844 seconds.\n",
      "Processing file 1020000 after 10764.363460540771 seconds.\n",
      "Processing file 1030000 after 10858.190533876419 seconds.\n",
      "Processing file 1040000 after 10949.926808595657 seconds.\n",
      "Processing file 1050000 after 11044.702885389328 seconds.\n",
      "Processing file 1060000 after 11138.702216863632 seconds.\n",
      "Processing file 1070000 after 11232.741785526276 seconds.\n",
      "Processing file 1080000 after 11323.983591079712 seconds.\n",
      "Processing file 1090000 after 11413.348184347153 seconds.\n",
      "Processing file 1100000 after 11503.998717069626 seconds.\n",
      "Processing file 1110000 after 11599.105439662933 seconds.\n",
      "Processing file 1120000 after 11688.691888809204 seconds.\n",
      "Processing file 1130000 after 11779.453407049179 seconds.\n",
      "Processing file 1140000 after 11873.189337968826 seconds.\n",
      "Processing file 1150000 after 11967.225532531738 seconds.\n",
      "Processing file 1160000 after 12060.717976808548 seconds.\n",
      "Processing file 1170000 after 12154.614139556885 seconds.\n",
      "Processing file 1180000 after 12250.868071079254 seconds.\n",
      "Processing file 1190000 after 12346.08429479599 seconds.\n",
      "Processing file 1200000 after 12441.032388925552 seconds.\n",
      "Processing file 1210000 after 12536.373236656189 seconds.\n",
      "Processing file 1220000 after 12628.086042881012 seconds.\n",
      "Processing file 1230000 after 12719.310064315796 seconds.\n",
      "Processing file 1240000 after 12816.705958843231 seconds.\n",
      "Processing file 1250000 after 12914.258068561554 seconds.\n",
      "Processing file 1260000 after 13005.089126348495 seconds.\n",
      "Processing file 1270000 after 13096.30085849762 seconds.\n",
      "Processing file 1280000 after 13188.06791806221 seconds.\n",
      "Processing file 1290000 after 13278.675970554352 seconds.\n",
      "Processing file 1300000 after 13368.463686466217 seconds.\n",
      "Processing file 1310000 after 13458.024767875671 seconds.\n",
      "Processing file 1320000 after 13548.072961330414 seconds.\n",
      "Processing file 1330000 after 13640.448924779892 seconds.\n",
      "Processing file 1340000 after 13733.472866535187 seconds.\n",
      "Processing file 1350000 after 13828.468445301056 seconds.\n",
      "Processing file 1360000 after 13921.496760606766 seconds.\n",
      "Processing file 1370000 after 14014.293028593063 seconds.\n",
      "Processing file 1380000 after 14105.78051185608 seconds.\n",
      "Processing file 1390000 after 14198.904641389847 seconds.\n",
      "Processing file 1400000 after 14294.16413640976 seconds.\n",
      "Processing file 1410000 after 14387.748620510101 seconds.\n",
      "Processing file 1420000 after 14482.050756692886 seconds.\n",
      "Processing file 1430000 after 14576.398681879044 seconds.\n",
      "Processing file 1440000 after 14669.582263469696 seconds.\n",
      "Processing file 1450000 after 14762.54050064087 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1460000 after 14854.470460414886 seconds.\n",
      "Processing file 1470000 after 14945.869807481766 seconds.\n",
      "Processing file 1480000 after 15042.709025144577 seconds.\n",
      "Processing file 1490000 after 15134.773734092712 seconds.\n",
      "Processing file 1500000 after 15229.693262577057 seconds.\n",
      "Processing file 1510000 after 15322.436637163162 seconds.\n",
      "Processing file 1520000 after 15415.914841651917 seconds.\n",
      "Processing file 1530000 after 15508.641138076782 seconds.\n",
      "Processing file 1540000 after 15600.45550107956 seconds.\n",
      "Processing file 1550000 after 15690.732978582382 seconds.\n",
      "Processing file 1560000 after 15782.111455917358 seconds.\n",
      "Processing file 1570000 after 15875.448921203613 seconds.\n",
      "Processing file 1580000 after 15967.985771656036 seconds.\n",
      "Processing file 1590000 after 16062.136303424835 seconds.\n",
      "Processing file 1600000 after 16156.769075155258 seconds.\n",
      "Processing file 1610000 after 16251.679577589035 seconds.\n",
      "Processing file 1620000 after 16343.595571279526 seconds.\n",
      "Processing file 1630000 after 16439.88295149803 seconds.\n",
      "Processing file 1640000 after 16532.89602613449 seconds.\n",
      "Processing file 1650000 after 16627.275145292282 seconds.\n",
      "Processing file 1660000 after 16722.125143289566 seconds.\n",
      "Processing file 1670000 after 16814.89864397049 seconds.\n",
      "Processing file 1680000 after 16909.746709108353 seconds.\n",
      "Processing file 1690000 after 17009.272212028503 seconds.\n",
      "Processing file 1700000 after 17107.33446264267 seconds.\n",
      "Processing file 1710000 after 17205.909540891647 seconds.\n",
      "Processing file 1720000 after 17303.811958551407 seconds.\n",
      "Processing file 1730000 after 17401.702825784683 seconds.\n",
      "Processing file 1740000 after 17500.19665646553 seconds.\n",
      "Processing file 1750000 after 17596.41210794449 seconds.\n",
      "Processing file 1760000 after 17697.4810795784 seconds.\n",
      "Processing file 1770000 after 17798.65202140808 seconds.\n",
      "Processing file 1780000 after 17904.341248989105 seconds.\n",
      "Processing file 1790000 after 18008.739812135696 seconds.\n",
      "Processing file 1800000 after 18113.77450156212 seconds.\n",
      "Processing file 1810000 after 18217.696189641953 seconds.\n",
      "Processing file 1820000 after 18322.8076107502 seconds.\n",
      "Processing file 1830000 after 18426.100841999054 seconds.\n",
      "Processing file 1840000 after 18531.90336871147 seconds.\n",
      "Processing file 1850000 after 18636.242387771606 seconds.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            sent_hede = score_article2(article.print_hede)\n",
    "            sent_lede = score_article2(article.lede)\n",
    "            sent_body = score_article2(article.paragraphs)\n",
    "            #sent_hede = 0\n",
    "            #sent_lede = 0\n",
    "            #sent_body = 0 # for now, let's not score the whole article, it takes so long\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1298960 train set files, 185565 dev set files, 77543 test set files.\n",
      "WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(all_files_count * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(all_files_count * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 308343 training files due to sentiment.\n"
     ]
    }
   ],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "drop_list = [] # holds the indices to drop\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        drop_list.append(row['filepath'])\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "\n",
    "filtered_df = train_df[~train_df.filepath.isin(drop_list)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# write (filtered) train log file\n",
    "filtered_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
