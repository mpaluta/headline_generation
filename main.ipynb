{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See shared google doc for buildout plan:\n",
    "https://docs.google.com/document/d/1GrwFtcygBsiHBWx3GpUJIW-Pr7bfTim9xVAksxTRZh8/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arnoldyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import statsmodels.formula.api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from NYT_parser import NYTArticle\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_TOTAL_FILES = 1855658 # The total number of XML files in the NYT Annotated corpus\n",
    "RANDOMIZATION_SEED = 100 # The seed is used to split data into train, dev, text in replicatable manner\n",
    "FIRST_PATH = '1994/02/02/0666100.xml' # Used as a check of the randomization\n",
    "TRAIN_SPLIT = 0.7 # Put 70% of the data into the train set\n",
    "DEV_SPLIT = 0.1 # Put 10% of the data into the dev set\n",
    "TEST_SPLIT = 0.2 # Put 20% of the data into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your directories should be set up as follows to run this notebook:\n",
    "# headline_generation (folder)\n",
    "#      |___ main.ipynb           The main notebook for training and showing test results\n",
    "#      |___ NYT_parser.ipynb     A class for parsing the raw XML files\n",
    "#      |___ utilities.ipynb      Some helper functions to keep code from getting cluttered\n",
    "#      |___ EDA.ipynb            Some initial exploratory data analysis work\n",
    "#      |___ Gavrilov.py          Tensor2Tensor subclass that defines our Problem \n",
    "#      |___ logs (folder)        Lists of filepaths based on various filters and train/dev/test split\n",
    "#      |___ data (folder)\n",
    "#             |___ sentiment (folder)\n",
    "#                    |___ positive-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#                    |___ negative-words.txt (unrar from: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "#             |___ glove (folder)\n",
    "#                    |___ glove.42B.300d.txt (unzip from: http://nlp.stanford.edu/data/glove.42B.300d.zip)\n",
    "#             |___ nyt (folder - unzip/untar from https://catalog.ldc.upenn.edu/download/a22bbeb044db7cb70954c21e130aec48c512cb90a2874a6746e3bc722b3f)\n",
    "#                    |___ 1987 (folders for all years from 1987 to 2007)\n",
    "#                           |___ 01 (folders for all months 1 to 12)\n",
    "#                                 |___ 01 (folders for all days of month)\n",
    "#                                       |___ 0000000.xml (1.8 million xml files numbered sequentially from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to setup the above structure\n",
    "\n",
    "# NOTE: Since some of the sites are password protected, this may not fully work automatically.\n",
    "# You will most likely need to do some manual downloading, unpacking, and moving of files.\n",
    "\n",
    "# WRITE THIS LATER AFTER ASKING PROFS HOW THEY WANT THIS HANDLED\n",
    "# THERE IS A TON OF MKDIR AND WGET CODE FROM OLD NOTEBOOKS THAT CAN BE ADAPTED\n",
    "# WE COULD ALSO DO A SET OF ASSERTS WHEN DEFINING FILE PATHS BELOW TO MAKE SURE DATA IS ORGANIZED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "nyt_path = './data/nyt/' # points to folder containing the years folders of the NYT Annotated corpus \n",
    "sentiment_path = './data/sentiment/' # points to folder containing sentiment classification data files\n",
    "glove_path = './data/glove/glove.42B.300d.txt' # point to file containing glove embeddings\n",
    "log_path = './logs/' # points to folder containing all the logs\n",
    "all_data_log = log_path + 'all_data.log' # points to file containing filepaths for all NYT xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log of all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a log file containing the names of all xml files in the corpus\n",
    "with open(all_data_log, 'w' ,encoding='utf-8', newline='') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    for root, dirs, files in sorted(os.walk(nyt_path)):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if nyt_path in filepath: # truncate the set path to NYT data in the log file\n",
    "                    filepath = filepath[filepath.find(nyt_path)+11:]\n",
    "                wr.writerow([filepath])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log with all xml filenames in and put into a list\n",
    "all_files_list = []\n",
    "with open(all_data_log, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            all_files_list.append(row[0])\n",
    "\n",
    "all_files_count = len(all_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged all 1855658 files in the NYT Annotated corpus.\n"
     ]
    }
   ],
   "source": [
    "# checksum\n",
    "if all_files_count == NYT_TOTAL_FILES:\n",
    "    print(\"You have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")\n",
    "else:\n",
    "    print(\"WARNING! You do not seem to have logged all\", NYT_TOTAL_FILES,\"files in the NYT Annotated corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings(glove_path) # load embeddigs\n",
    "pos_words = load_lexicon(sentiment_path+'positive-words.txt')\n",
    "neg_words = load_lexicon(sentiment_path+'negative-words.txt')\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for sentiment analysis \n",
    "\n",
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    try: \n",
    "        sentiments = words_to_sentiment(tokens)\n",
    "    except: # handle case where there's no known words in input\n",
    "        return 0\n",
    "    return sentiments['sentiment'].mean()\n",
    "\n",
    "def score_article(text):\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sentence in text:\n",
    "        num_sentences += 1\n",
    "        score += text_to_sentiment(sentence)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10000 after 5.651543140411377 seconds.\n",
      "Processing file 20000 after 11.488011360168457 seconds.\n",
      "Processing file 30000 after 17.12196969985962 seconds.\n",
      "Processing file 40000 after 22.781628131866455 seconds.\n",
      "Processing file 50000 after 28.366264581680298 seconds.\n",
      "Processing file 60000 after 34.27134847640991 seconds.\n",
      "Processing file 70000 after 40.26205801963806 seconds.\n",
      "Processing file 80000 after 46.275816917419434 seconds.\n",
      "Processing file 90000 after 52.14109754562378 seconds.\n",
      "Processing file 100000 after 57.830689668655396 seconds.\n",
      "Processing file 110000 after 63.48590612411499 seconds.\n",
      "Processing file 120000 after 69.06751036643982 seconds.\n",
      "Processing file 130000 after 74.86501717567444 seconds.\n",
      "Processing file 140000 after 80.81443405151367 seconds.\n",
      "Processing file 150000 after 86.55361557006836 seconds.\n",
      "Processing file 160000 after 92.55700254440308 seconds.\n",
      "Processing file 170000 after 98.54177641868591 seconds.\n",
      "Processing file 180000 after 104.37856936454773 seconds.\n",
      "Processing file 190000 after 110.03179979324341 seconds.\n",
      "Processing file 200000 after 116.0083749294281 seconds.\n",
      "Processing file 210000 after 121.48431658744812 seconds.\n",
      "Processing file 220000 after 126.96615672111511 seconds.\n",
      "Processing file 230000 after 132.4456753730774 seconds.\n",
      "Processing file 240000 after 138.0756757259369 seconds.\n",
      "Processing file 250000 after 143.97316360473633 seconds.\n",
      "Processing file 260000 after 149.63611721992493 seconds.\n",
      "Processing file 270000 after 155.14815664291382 seconds.\n",
      "Processing file 280000 after 160.71414494514465 seconds.\n",
      "Processing file 290000 after 166.2019386291504 seconds.\n",
      "Processing file 300000 after 171.83721470832825 seconds.\n",
      "Processing file 310000 after 177.48577189445496 seconds.\n",
      "Processing file 320000 after 183.07051610946655 seconds.\n",
      "Processing file 330000 after 188.77504801750183 seconds.\n",
      "Processing file 340000 after 194.4056282043457 seconds.\n",
      "Processing file 350000 after 200.14236640930176 seconds.\n",
      "Processing file 360000 after 205.7984037399292 seconds.\n",
      "Processing file 370000 after 211.66668009757996 seconds.\n",
      "Processing file 380000 after 217.59448623657227 seconds.\n",
      "Processing file 390000 after 223.20737433433533 seconds.\n",
      "Processing file 400000 after 229.29306554794312 seconds.\n",
      "Processing file 410000 after 235.29997777938843 seconds.\n",
      "Processing file 420000 after 241.39582896232605 seconds.\n",
      "Processing file 430000 after 247.32929110527039 seconds.\n",
      "Processing file 440000 after 253.40733981132507 seconds.\n",
      "Processing file 450000 after 258.9033625125885 seconds.\n",
      "Processing file 460000 after 264.3071994781494 seconds.\n",
      "Processing file 470000 after 269.8967778682709 seconds.\n",
      "Processing file 480000 after 275.379230260849 seconds.\n",
      "Processing file 490000 after 280.86597061157227 seconds.\n",
      "Processing file 500000 after 286.3469080924988 seconds.\n",
      "Processing file 510000 after 291.8510353565216 seconds.\n",
      "Processing file 520000 after 297.4940416812897 seconds.\n",
      "Processing file 530000 after 303.0365569591522 seconds.\n",
      "Processing file 540000 after 308.61865067481995 seconds.\n",
      "Processing file 550000 after 314.34393644332886 seconds.\n",
      "Processing file 560000 after 319.8601040840149 seconds.\n",
      "Processing file 570000 after 325.9273450374603 seconds.\n",
      "Processing file 580000 after 331.4393391609192 seconds.\n",
      "Processing file 590000 after 336.91702032089233 seconds.\n",
      "Processing file 600000 after 342.41663432121277 seconds.\n",
      "Processing file 610000 after 347.9088969230652 seconds.\n",
      "Processing file 620000 after 353.6416299343109 seconds.\n",
      "Processing file 630000 after 359.12764716148376 seconds.\n",
      "Processing file 640000 after 364.5707528591156 seconds.\n",
      "Processing file 650000 after 370.10347747802734 seconds.\n",
      "Processing file 660000 after 375.6510362625122 seconds.\n",
      "Processing file 670000 after 381.12653851509094 seconds.\n",
      "Processing file 680000 after 386.7813184261322 seconds.\n",
      "Processing file 690000 after 392.2459409236908 seconds.\n",
      "Processing file 700000 after 397.7026925086975 seconds.\n",
      "Processing file 710000 after 403.22090101242065 seconds.\n",
      "Processing file 720000 after 408.72902512550354 seconds.\n",
      "Processing file 730000 after 414.4739992618561 seconds.\n",
      "Processing file 740000 after 419.90595960617065 seconds.\n",
      "Processing file 750000 after 425.31439232826233 seconds.\n",
      "Processing file 760000 after 430.7699148654938 seconds.\n",
      "Processing file 770000 after 436.2109889984131 seconds.\n",
      "Processing file 780000 after 441.6458332538605 seconds.\n",
      "Processing file 790000 after 447.0433371067047 seconds.\n",
      "Processing file 800000 after 452.4668836593628 seconds.\n",
      "Processing file 810000 after 457.8487014770508 seconds.\n",
      "Processing file 820000 after 463.241028547287 seconds.\n",
      "Processing file 830000 after 468.6875431537628 seconds.\n",
      "Processing file 840000 after 474.1226387023926 seconds.\n",
      "Processing file 850000 after 479.5609555244446 seconds.\n",
      "Processing file 860000 after 485.01680183410645 seconds.\n",
      "Processing file 870000 after 490.4913532733917 seconds.\n",
      "Processing file 880000 after 496.03603076934814 seconds.\n",
      "Processing file 890000 after 501.7804138660431 seconds.\n",
      "Processing file 900000 after 507.2869219779968 seconds.\n",
      "Processing file 910000 after 512.7042381763458 seconds.\n",
      "Processing file 920000 after 518.1337294578552 seconds.\n",
      "Processing file 930000 after 523.5464231967926 seconds.\n",
      "Processing file 940000 after 528.9888679981232 seconds.\n",
      "Processing file 950000 after 534.4426724910736 seconds.\n",
      "Processing file 960000 after 539.8666751384735 seconds.\n",
      "Processing file 970000 after 545.3449990749359 seconds.\n",
      "Processing file 980000 after 550.8100125789642 seconds.\n",
      "Processing file 990000 after 556.2986633777618 seconds.\n",
      "Processing file 1000000 after 561.8410041332245 seconds.\n",
      "Processing file 1010000 after 567.3180146217346 seconds.\n",
      "Processing file 1020000 after 572.7994391918182 seconds.\n",
      "Processing file 1030000 after 578.2491698265076 seconds.\n",
      "Processing file 1040000 after 583.7498304843903 seconds.\n",
      "Processing file 1050000 after 589.2567164897919 seconds.\n",
      "Processing file 1060000 after 595.0283451080322 seconds.\n",
      "Processing file 1070000 after 600.5429549217224 seconds.\n",
      "Processing file 1080000 after 606.0558168888092 seconds.\n",
      "Processing file 1090000 after 611.5584855079651 seconds.\n",
      "Processing file 1100000 after 617.0524005889893 seconds.\n",
      "Processing file 1110000 after 622.605066537857 seconds.\n",
      "Processing file 1120000 after 628.0939688682556 seconds.\n",
      "Processing file 1130000 after 633.5555882453918 seconds.\n",
      "Processing file 1140000 after 639.0455477237701 seconds.\n",
      "Processing file 1150000 after 644.5036029815674 seconds.\n",
      "Processing file 1160000 after 649.9118473529816 seconds.\n",
      "Processing file 1170000 after 655.2825591564178 seconds.\n",
      "Processing file 1180000 after 660.7364356517792 seconds.\n",
      "Processing file 1190000 after 666.2159354686737 seconds.\n",
      "Processing file 1200000 after 671.6155338287354 seconds.\n",
      "Processing file 1210000 after 677.0273787975311 seconds.\n",
      "Processing file 1220000 after 682.4653778076172 seconds.\n",
      "Processing file 1230000 after 687.8895156383514 seconds.\n",
      "Processing file 1240000 after 693.3699517250061 seconds.\n",
      "Processing file 1250000 after 698.8608024120331 seconds.\n",
      "Processing file 1260000 after 704.30837225914 seconds.\n",
      "Processing file 1270000 after 709.7378575801849 seconds.\n",
      "Processing file 1280000 after 715.4378311634064 seconds.\n",
      "Processing file 1290000 after 721.1595485210419 seconds.\n",
      "Processing file 1300000 after 726.923344373703 seconds.\n",
      "Processing file 1310000 after 732.4978268146515 seconds.\n",
      "Processing file 1320000 after 738.1139922142029 seconds.\n",
      "Processing file 1330000 after 743.9687781333923 seconds.\n",
      "Processing file 1340000 after 749.8443574905396 seconds.\n",
      "Processing file 1350000 after 755.6409356594086 seconds.\n",
      "Processing file 1360000 after 761.3782801628113 seconds.\n",
      "Processing file 1370000 after 767.1377792358398 seconds.\n",
      "Processing file 1380000 after 772.8227834701538 seconds.\n",
      "Processing file 1390000 after 778.496518611908 seconds.\n",
      "Processing file 1400000 after 784.1940741539001 seconds.\n",
      "Processing file 1410000 after 789.8957316875458 seconds.\n",
      "Processing file 1420000 after 795.6325025558472 seconds.\n",
      "Processing file 1430000 after 801.3603751659393 seconds.\n",
      "Processing file 1440000 after 807.1138789653778 seconds.\n",
      "Processing file 1450000 after 812.8007843494415 seconds.\n",
      "Processing file 1460000 after 818.4663517475128 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1470000 after 824.2130124568939 seconds.\n",
      "Processing file 1480000 after 830.0862579345703 seconds.\n",
      "Processing file 1490000 after 835.8752624988556 seconds.\n",
      "Processing file 1500000 after 841.5952184200287 seconds.\n",
      "Processing file 1510000 after 847.3412425518036 seconds.\n",
      "Processing file 1520000 after 853.0577564239502 seconds.\n",
      "Processing file 1530000 after 858.7690873146057 seconds.\n",
      "Processing file 1540000 after 864.4760789871216 seconds.\n",
      "Processing file 1550000 after 870.150705575943 seconds.\n",
      "Processing file 1560000 after 875.863137960434 seconds.\n",
      "Processing file 1570000 after 881.613205909729 seconds.\n",
      "Processing file 1580000 after 887.5466992855072 seconds.\n",
      "Processing file 1590000 after 893.2559015750885 seconds.\n",
      "Processing file 1600000 after 898.9979708194733 seconds.\n",
      "Processing file 1610000 after 904.7520141601562 seconds.\n",
      "Processing file 1620000 after 910.4812188148499 seconds.\n",
      "Processing file 1630000 after 916.291600227356 seconds.\n",
      "Processing file 1640000 after 921.9732296466827 seconds.\n",
      "Processing file 1650000 after 927.6814591884613 seconds.\n",
      "Processing file 1660000 after 933.4737915992737 seconds.\n",
      "Processing file 1670000 after 939.1314797401428 seconds.\n",
      "Processing file 1680000 after 944.5884292125702 seconds.\n",
      "Processing file 1690000 after 950.0373730659485 seconds.\n",
      "Processing file 1700000 after 955.5018944740295 seconds.\n",
      "Processing file 1710000 after 960.9423203468323 seconds.\n",
      "Processing file 1720000 after 966.4526455402374 seconds.\n",
      "Processing file 1730000 after 972.0237536430359 seconds.\n",
      "Processing file 1740000 after 977.61523604393 seconds.\n",
      "Processing file 1750000 after 983.0865819454193 seconds.\n",
      "Processing file 1760000 after 988.7720172405243 seconds.\n",
      "Processing file 1770000 after 994.5134015083313 seconds.\n",
      "Processing file 1780000 after 1000.2399282455444 seconds.\n",
      "Processing file 1790000 after 1005.973438501358 seconds.\n",
      "Processing file 1800000 after 1011.6809186935425 seconds.\n",
      "Processing file 1810000 after 1017.3235263824463 seconds.\n",
      "Processing file 1820000 after 1023.1099951267242 seconds.\n",
      "Processing file 1830000 after 1028.8790316581726 seconds.\n",
      "Processing file 1840000 after 1034.6087000370026 seconds.\n",
      "Processing file 1850000 after 1040.27153134346 seconds.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TAKES A LONG TIME TO RUN\n",
    "with open(log_path+\"meta_data.log\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile)\n",
    "    # set time\n",
    "    start_time = time.time()\n",
    "    # set counter\n",
    "    filecount = 0\n",
    "    for filepath in all_files_list:\n",
    "        filecount += 1\n",
    "        if (filecount%10000 == 0):\n",
    "            print(\"Processing file\",filecount,\"after\",time.time() - start_time, \"seconds.\")\n",
    "        # get NYT meta data\n",
    "        article = NYTArticle.from_file(os.path.join(nyt_path, filepath))\n",
    "        if article.pass_filters(): # applies Gavrilov's basic filtering: no obits, hedes and body text within a wordcount range\n",
    "            hede_size, section, wordcount = article.get_meta()\n",
    "            # calc sentiment data\n",
    "            #sent_hede = score_article(article.print_hede)\n",
    "            #sent_lede = score_article(article.lede)\n",
    "            #sent_body = score_article(article.paragraphs)\n",
    "            sent_hede = 0\n",
    "            sent_lede = 0\n",
    "            sent_body = 0 # for now, let's not score the whole article, it takes so long\n",
    "            # write row of meta data        \n",
    "            wr.writerow([filepath, hede_size, wordcount, section, sent_hede, sent_lede, sent_body])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1298960 train set files, 185565 dev set files, 77659 test set files.\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"filepath\", \"hede_size\", \"wordcount\", \"section\", \"sent_hede\", \"sent_lede\", \"sent_body\"]\n",
    "\n",
    "# read the meta log into a pd.DataFrame\n",
    "meta_df = pd.read_csv(log_path+\"meta_data.log\", sep=\",\", header=None, names=colnames, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# check if splits are rational\n",
    "if (TRAIN_SPLIT + DEV_SPLIT + TEST_SPLIT) != 1.0:\n",
    "    print(\"WARNING! Your train/dev/test splits do not toal 1.0.\")\n",
    "\n",
    "# shuffle!\n",
    "rando = np.random.seed(seed=RANDOMIZATION_SEED)\n",
    "meta_df = meta_df.sample(frac=1, axis=0, random_state=rando).reset_index(drop=True) # this shuffles randomly\n",
    "\n",
    "# set breaks\n",
    "train_break = int(all_files_count * TRAIN_SPLIT)\n",
    "dev_break = train_break + int(all_files_count * DEV_SPLIT) # rest is test\n",
    "\n",
    "# split the train, dev, test sets\n",
    "train_df = meta_df[0:train_break]\n",
    "dev_df = meta_df[train_break:dev_break]\n",
    "test_df = meta_df[dev_break:]\n",
    "\n",
    "# output number of files in each split\n",
    "print(\"There are\",len(train_df),\"train set files,\",len(dev_df),\"dev set files,\",len(test_df),\"test set files.\")\n",
    "\n",
    "# checksum\n",
    "if train_df['filepath'][0] != FIRST_PATH:\n",
    "    print(\"WARNING! Your randomization differs. This will leave you with different train/dev/test splits.\")\n",
    "    \n",
    "# write the split data to individual meta log files\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train_unfltrd.log\", index=False, header=True)\n",
    "dev_df.to_csv(path_or_buf=log_path+\"meta_dev.log\", index=False, header=True)\n",
    "test_df.to_csv(path_or_buf=log_path+\"meta_test.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 training files due to sentiment.\n"
     ]
    }
   ],
   "source": [
    "# load in unfiltered train log file\n",
    "train_df = pd.read_csv(log_path+\"meta_train_unfltrd.log\", sep=\",\", header=0, \n",
    "                 dtype={'filepath': str,'hede_size': int,'wordcount': int,'section': str, 'sent_hede': float, 'sent_lede': float, 'sent_body': float})\n",
    "\n",
    "# Conduct sentiment filtering on the train data\n",
    "# INITIAL TEST: filter out all articles that have opposite sentiment in headline and lede\n",
    "drop_count = 0\n",
    "for index, row in train_df.iterrows():\n",
    "    headline = float(row['sent_hede'])\n",
    "    lede = float(row['sent_lede'])\n",
    "    if (headline > 0 and lede < 0) or (headline < 0 and lede > 0):\n",
    "        drop_count += 1\n",
    "        train_df.drop(index, inplace=True)\n",
    "print(\"Filtered out\", drop_count,\"training files due to sentiment.\")\n",
    "        \n",
    "# write (filtered) train log file\n",
    "train_df.to_csv(path_or_buf=log_path+\"meta_train.log\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
